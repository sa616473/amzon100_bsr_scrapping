{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INITIAL_URL = \"https://www.amazon.com/Best-Sellers/zgbs/ref=zg_bs_unv_0_boost_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def department_scrapping():\n",
    "    #initializing selium\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(INITIAL_URL)\n",
    "    src=driver.page_source\n",
    "    soup=BeautifulSoup(src,'html.parser')\n",
    "    list_of_departments = soup.findAll('ul',{'id':'zg_browseRoot'})\n",
    "    links_to_bsr_dep = []\n",
    "    dep = []\n",
    "    for department in list_of_departments:\n",
    "        a_tag = department.findAll('a')\n",
    "    for tag in a_tag:\n",
    "        dep.append(tag.text)\n",
    "#         print(tag['href'])\n",
    "        links_to_bsr_dep.append(tag['href'])\n",
    "    dep_dict = {'department': dep, 'URL': links_to_bsr_dep}\n",
    "    temp = pd.DataFrame(dep_dict)\n",
    "    if not os.path.exists(\"./departments\"):\n",
    "        os.makedirs(\"./departments\")\n",
    "        temp.to_csv(\"./departments/department.csv\")\n",
    "    else:\n",
    "        temp.to_csv(\"./departments/department.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_scrapping():\n",
    "    df = pd.read_csv('./departments/department.csv')\n",
    "    driver = webdriver.Chrome()\n",
    "    for department,url in zip(df['department'],df['URL']):\n",
    "#         print(url)\n",
    "        driver.get(url)\n",
    "        src=driver.page_source\n",
    "        soup=BeautifulSoup(src,'html.parser')\n",
    "        list_of_cat = soup.findAll('ul',{'id':'zg_browseRoot'})\n",
    "        cat_text = []\n",
    "        cat_links = []\n",
    "        for cat in list_of_cat:\n",
    "            cat_tag = cat.findAll('a')\n",
    "        for cat in cat_tag[1:]:\n",
    "            cat_text.append(cat.text)\n",
    "            cat_links.append(cat['href'])\n",
    "        cat_dict = {'category': cat_text, 'URL': cat_links}\n",
    "        temp = pd.DataFrame(cat_dict)\n",
    "        if not os.path.exists(\"./categories\"):\n",
    "            os.makedirs(\"./categories\")\n",
    "            temp.to_csv(\"./categories/{}.csv\".format(department))\n",
    "        else:\n",
    "            temp.to_csv(\"./categories/{}.csv\".format(department))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_category_scrapping():\n",
    "    departments = pd.read_csv(\"./departments/department.csv\")\n",
    "    driver = webdriver.Chrome()\n",
    "    for dep in departments[\"department\"]:\n",
    "        df = pd.read_csv(\"./categories/{}.csv\".format(dep))\n",
    "        for cat,url in zip(df[\"category\"],df[\"URL\"]):\n",
    "            print(cat)\n",
    "            print(url)\n",
    "            driver.get(url)\n",
    "            src=driver.page_source\n",
    "            soup=BeautifulSoup(src,'html.parser')\n",
    "            list_of_sub_cat = soup.findAll('ul',{'id':'zg_browseRoot'})\n",
    "            sub_cat_text = []\n",
    "            sub_cat_links = []\n",
    "\n",
    "            for sub_cat in list_of_sub_cat:\n",
    "                sub_cat_tag = sub_cat.findAll('a')\n",
    "\n",
    "            for sub_cat in sub_cat_tag[1:]:\n",
    "                sub_cat_text.append(sub_cat.text)\n",
    "                sub_cat_links.append(sub_cat['href'])\n",
    "\n",
    "            sub_cat_dict = {'category': sub_cat_text, 'URL': sub_cat_links}\n",
    "            temp = pd.DataFrame(sub_cat_dict)\n",
    "            if not os.path.exists(\"./sub_cat/{}\".format(cat)):\n",
    "                os.makedirs(\"./sub_cat/{}\".format(cat))\n",
    "                temp.to_csv(\"./sub_cat/{}\".format(cat) + \"/{}.csv\".format(cat))\n",
    "            else:\n",
    "                temp.to_csv(\"./sub_cat/{}\".format(cat) + \"/{}.csv\".format(cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_100_scrapping():\n",
    "    sub_categories = os.listdir('./sub_cat/')\n",
    "    driver = webdriver.Chrome()\n",
    "    for sub_cat in sub_categories:\n",
    "        df =pd.read_csv(\"./sub_cat/{}/{}.csv\".format(sub_cat,sub_cat))\n",
    "        for cat,url in zip(df['category'],df['URL']):\n",
    "            driver.get(url)\n",
    "            src=driver.page_source\n",
    "            soup=BeautifulSoup(src,'html.parser')\n",
    "            listitem = soup.findAll('ol',{'id':'zg-ordered-list'})\n",
    "            item_name = []\n",
    "            item_link = []\n",
    "            category = []\n",
    "            for item in listitem:\n",
    "                item_tag = item.findAll('a',{'class':'a-link-normal'})\n",
    "#                 print(item_tag[0])\n",
    "            for item in item_tag:\n",
    "                item_name.append(item['href'].split('/')[1])\n",
    "                item_link.append(item['href'])\n",
    "                category.append(cat)\n",
    "            item_dict = {'sub_cat':category,'item_name':item_name, 'URL':item_link}\n",
    "            temp = pd.DataFrame(item_dict)\n",
    "            if not os.path.exists(\"./top100/{}\".format(cat)):\n",
    "                os.makedirs(\"./top100/{}\".format(cat))\n",
    "                temp.to_csv(\"./top100/{}/top_100{}.csv\".format(cat,cat))\n",
    "            else:\n",
    "                temp.to_csv(\"./top100/{}/top_100{}.csv\".format(cat,cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_scrapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_scrapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_category_scrapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_scrapping()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
